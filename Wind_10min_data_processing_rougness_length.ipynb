{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a81c065e",
   "metadata": {},
   "source": [
    "### What this script does\n",
    "\n",
    "- Loads all merged_data_10min.csv from your corresponding folders (Mar–Jun 2024).\n",
    "\n",
    "- Estimates surface roughness length z₀ in multiple ways:\n",
    "\n",
    "- simple log-law fit, 2) stability-corrected (Monin–Obukhov) fit using u_* and L.\n",
    "\n",
    "- Computes fit-quality (R²), filters high-quality cases, and makes time series + PDF/KDE plots.\n",
    "\n",
    "- Breaks z₀ distributions down by wind direction sectors (N, NE, …, NW).\n",
    "\n",
    "- Summarizes stats (mean/median/std/IQR, percentiles, MAD, skewness, kurtosis, outlier counts).\n",
    "\n",
    "#### Edit these lines before running\n",
    "- Base directory containing monthly 10-min dataset folders (containing wind from mast and sonic and fluxes)\n",
    "  \n",
    "  base_dir = r\"C:\\path\\to\\your\\Sonic\"\n",
    "\n",
    "- Which months to process: months = ['2024-03', '2024-04', '2024-05', '2024-06']\n",
    "\n",
    "- Measurement heights used in the profile fits (meters) — change if your sensor heights differ:   heights = np.array([2.0, 4.47, 10.0])\n",
    "\n",
    "- Quality and filtering thresholds used in plots/statistics\n",
    "\n",
    "- Wind-direction column name, if your file uses a different one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015b119f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "!pip install pvlib\n",
    "from sklearn.metrics import r2_score\n",
    "from matplotlib.dates import DateFormatter\n",
    "from datetime import time\n",
    "import pvlib\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.dates as mdates\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488cf52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "Cp = 1005  # Specific heat capacity of dry air at constant pressure (J/kg/K)\n",
    "g = 9.81   # Acceleration due to gravity (m/s^2)\n",
    "\n",
    "A=6.11*100 #Pa\n",
    "beta=0.067 #K^-1\n",
    "Ttrip=27\n",
    "3.16 #K\n",
    "epsilon=0.622\n",
    "sigma=5.67e-8 #W*m^-2*K^-4\n",
    "Lv = 2.5e6  # Latent heat of vaporization in J/kg\n",
    "kappa=0.4\n",
    "Rd=287.04\n",
    "Rv=461.5\n",
    "\n",
    "rho_atm=1.225 #kg/m^3\n",
    "m_co2=0.044 #kg/mole molecular mass CO2\n",
    "m_atm=0.028 #molecular mass atmosphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d38a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def psi_m(zeta):\n",
    "    if zeta < -3/500:  # Unstable\n",
    "        x = (1 - 16 * zeta) ** 0.25\n",
    "        return 2 * np.log((1 + x) / 2) + np.log((1 + x**2) / 2) - 2 * np.arctan(x) + np.pi / 2\n",
    "    elif zeta > 3/500:  # Stable\n",
    "        return -5 * zeta\n",
    "    else:\n",
    "        return 0  # Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d80435",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Edit these lines before running!!!\n",
    "# Base directory containing monthly 10-min dataset folders (containing wind from mast and sonic and fluxes)\n",
    "base_dir = r\"C:\\path\\to\\your\\Sonic\"\n",
    "\n",
    "# List of months to process\n",
    "months = ['2024-03', '2024-04', '2024-05', '2024-06']\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "all_data = []\n",
    "\n",
    "# Loop through each month's folder\n",
    "for month in months:\n",
    "    month_dir = os.path.join(base_dir, month)\n",
    "    \n",
    "    # Check if the monthly directory exists\n",
    "    if not os.path.exists(month_dir):\n",
    "        print(f\"Directory does not exist: {month_dir}\")\n",
    "        continue\n",
    "    \n",
    "    # Loop through each day's folder in the month\n",
    "    for day in os.listdir(month_dir):\n",
    "        day_dir = os.path.join(month_dir, day)\n",
    "        \n",
    "        # Check if it's a directory\n",
    "        if not os.path.isdir(day_dir):\n",
    "            continue\n",
    "        \n",
    "        # Define the path to the 'merged_data_10min.csv' file\n",
    "        file_path = os.path.join(day_dir, 'merged_data_10min.csv')\n",
    "        \n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file_path):\n",
    "            # Load the data and append it to the list\n",
    "            try:\n",
    "                data = pd.read_csv(file_path)\n",
    "                all_data.append(data)\n",
    "                print(f\"Loaded data from: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file {file_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"File does not exist: {file_path}\")\n",
    "\n",
    "# Combine all the data into one DataFrame\n",
    "if all_data:\n",
    "    combined_data = pd.concat(all_data, ignore_index=True)\n",
    "    print(\"Successfully combined all data.\")\n",
    "else:\n",
    "    combined_data = pd.DataFrame()  # Empty DataFrame if no data found\n",
    "    print(\"No data files found.\")\n",
    "\n",
    "# Output the first few rows of the combined DataFrame\n",
    "print(combined_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bd9ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6380db8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "heights = np.array([2, 4.47, 10])  # in meters\n",
    "\n",
    "# Prepare a new column for roughness length (z0)\n",
    "merged_data_10min['z0'] = np.nan\n",
    "\n",
    "\n",
    "# Exponential model to fit: wind_speed = a * exp(b * log(height))\n",
    "def exponential_model(log_height, a, b):\n",
    "    return a * np.exp(b * log_height)\n",
    "\n",
    "# Loop through each timestamp to calculate z0\n",
    "for index, row in merged_data_10min.iterrows():\n",
    "    # Wind speeds at the corresponding heights\n",
    "    wind_speeds = np.array([\n",
    "        row['WS_ms_D15008_Avg'],  # 2m\n",
    "        row['WS_ms_D15014_Avg'],  # 4.47m\n",
    "        row['WS_ms_D15463_Avg']   # 10m\n",
    "    ])\n",
    "\n",
    "    # Ensure all wind speeds are valid\n",
    "    if np.all(np.isfinite(wind_speeds)):\n",
    "        # Log-transform the heights\n",
    "        log_heights = np.log(heights)\n",
    "\n",
    "        # Fit the exponential model to log_heights and wind_speeds\n",
    "        popt, _ = curve_fit(exponential_model, log_heights, wind_speeds, maxfev=10000)\n",
    "\n",
    "        # Extract parameters a and b\n",
    "        a, b = popt\n",
    "\n",
    "        # Calculate roughness length z0 by solving for height where wind speed approaches zero\n",
    "        # Rearranged: 0 = a * exp(b * log(z0)) -> log(z0) = -a / b\n",
    "        log_z0 = -np.log(a) / b\n",
    "        z0 = np.exp(log_z0)  # Convert from log scale to linear\n",
    "\n",
    "        # Save z0 in the DataFrame\n",
    "        merged_data_10min.at[index, 'z0'] = z0\n",
    "merged_data_10min['TIMESTAMP'] = pd.to_datetime(merged_data_10min['TIMESTAMP'], errors='coerce')\n",
    "\n",
    "# Plot z0 vs. time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(merged_data_10min['TIMESTAMP'], merged_data_10min['z0'], marker='o', linestyle='-', color='blue')\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.ylabel('Roughness Length (z0) [m]', fontsize=14)\n",
    "plt.title('Roughness Length (z0) vs. Time', fontsize=16)\n",
    "plt.xticks(rotation=45)\n",
    "#plt.grid(True)\n",
    "plt.tight_layout()\n",
    "#plt.ylim(0,2)\n",
    "\n",
    "# Save the plot\n",
    "#plt.savefig('roughness_length_vs_time.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Display the roughness lengths in the DataFrame\n",
    "print(merged_data_10min[['TIMESTAMP', 'z0']])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b0fa7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Heights of measurement points in meters\n",
    "heights = np.array([2, 4.47, 10])\n",
    "# Log-transform the heights\n",
    "log_heights = np.log(heights)\n",
    "# Prepare a new column for roughness length (z0) in the combined DataFrame\n",
    "combined_data['z0'] = np.nan\n",
    "combined_data['z0_corrected'] = np.nan\n",
    "    \n",
    "# Compute u_star and L before looping\n",
    "combined_data['u_star'] = ((combined_data['uw_flux_corr']**2 + combined_data['vw_flux_corr']**2)**0.5)**0.5\n",
    "combined_data['L'] = - (combined_data['u_star']**3) / (\n",
    "    (kappa * g / combined_data['Average_Temperature_Corr']) * combined_data['wT_Flux']\n",
    ")\n",
    "# Display the roughness lengths in the DataFrame\n",
    "print(combined_data[['TIMESTAMP', 'z0','u_star' ,'WindDir_D15463_Avg']])\n",
    "# Exponential model to fit: wind_speed = a * exp(b * log(height))\n",
    "def exponential_model(log_height, a, b):\n",
    "    return a * np.exp(b * log_height)\n",
    "\n",
    "# Loop through each timestamp to calculate z0\n",
    "for index, row in combined_data.iterrows():\n",
    "    # Wind speeds at the corresponding heights\n",
    "    wind_speeds = np.array([\n",
    "        row['WS_ms_D15008_Avg'],  # 2m\n",
    "        row['WS_ms_D15014_Avg'],  # 4.47m\n",
    "        row['WS_ms_D15463_Avg']   # 10m\n",
    "    ])\n",
    "\n",
    "    # Check for NaN values in wind speeds\n",
    "    if not np.all(np.isfinite(wind_speeds)):\n",
    "        continue\n",
    "    \n",
    "   \n",
    "\n",
    "    # Fit the exponential model to log_heights and wind_speeds\n",
    "    try:\n",
    "        popt, _ = curve_fit(exponential_model, log_heights, wind_speeds, maxfev=10000)\n",
    "\n",
    "        # Extract parameters a and b\n",
    "        a, b = popt\n",
    "\n",
    "        # Calculate roughness length z0 by solving for height where wind speed approaches zero\n",
    "        log_z0 = -np.log(a) / b\n",
    "        z0 = np.exp(log_z0)  # Convert from log scale to linear\n",
    "\n",
    "        # Save z0 in the DataFrame\n",
    "        combined_data.at[index, 'z0'] = z0\n",
    "    except RuntimeError:\n",
    "        # If the curve fit fails, skip this row\n",
    "        print(f\"Curve fitting failed at index {index}, skipping row.\")\n",
    "        continue\n",
    "\n",
    "\n",
    "# Iterate over each row\n",
    "for index, row in combined_data.iterrows():\n",
    "    wind_speeds = np.array([\n",
    "        row['WS_ms_D15008_Avg'],  # 2m\n",
    "        row['WS_ms_D15014_Avg'],  # 4.47m\n",
    "        row['WS_ms_D15463_Avg']   # 10m\n",
    "    ])\n",
    "\n",
    "    if not np.all(np.isfinite(wind_speeds)):\n",
    "        continue\n",
    "\n",
    "    u_star = row['u_star']\n",
    "    L = row['L']\n",
    "    if u_star <= 0 or not np.isfinite(L):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        def wind_profile(z, z0):\n",
    "            zeta = 3 / L\n",
    "            return (u_star / kappa) * (np.log(z / z0) - psi_m(zeta))\n",
    "\n",
    "        popt, _ = curve_fit(wind_profile, heights, wind_speeds, bounds=(1e-4, 10))\n",
    "        #combined_data.at[index, 'z0_corrected'] = popt[0]\n",
    "        z0_corr = popt[0]\n",
    "        combined_data.at[index, 'z0_corrected'] = z0_corr\n",
    "\n",
    "        # Predict wind speeds and calculate R²\n",
    "        predicted_speeds = wind_profile(heights, z0_corr)\n",
    "        r2_corr = r2_score(wind_speeds, predicted_speeds)\n",
    "        combined_data.at[index, 'z0_corr_r2'] = r2_corr\n",
    "\n",
    "    except:\n",
    "        continue\n",
    "# Ensure the TIMESTAMP column is in datetime format\n",
    "combined_data['TIMESTAMP'] = pd.to_datetime(combined_data['TIMESTAMP'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN in z0 or WindDir_D15463_Avg\n",
    "combined_data.dropna(subset=['z0','z0_corrected','WindDir_D15463_Avg'], inplace=True)\n",
    "\n",
    "# Plot z0 vs. time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(combined_data['TIMESTAMP'], combined_data['z0'], marker='o', color='blue')\n",
    "plt.plot(combined_data['TIMESTAMP'], combined_data['z0_corrected'], label='Stability-corrected z0', alpha=0.8)\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.ylabel('Roughness Length (z0) [m]', fontsize=14)\n",
    "plt.title('Roughness Length (z0) vs. Time', fontsize=16)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828c3b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaNs from the R² column\n",
    "valid_r2 = combined_data['z0_corr_r2'].dropna()\n",
    "\n",
    "# Calculate metrics\n",
    "total = len(valid_r2)\n",
    "high_quality = (valid_r2 > 0.9).sum()\n",
    "pct_high_quality = 100 * high_quality / total if total > 0 else 0\n",
    "\n",
    "# Print results\n",
    "print(f\"Total z0_corrected fits evaluated: {total}\")\n",
    "print(f\"Fits with R² > 0.9: {high_quality} ({pct_high_quality:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3c511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter the data\n",
    "filtered_z0 = combined_data[(combined_data['z0_corr_r2'] > 0.9) & (combined_data['z0_corrected'] < 1)]['z0_corrected']\n",
    "\n",
    "# Compute metrics\n",
    "mean_z0 = filtered_z0.mean()\n",
    "median_z0 = filtered_z0.median()\n",
    "std_z0 = filtered_z0.std()\n",
    "iqr_z0 = filtered_z0.quantile(0.75) - filtered_z0.quantile(0.25)\n",
    "percentiles = filtered_z0.quantile([0.10, 0.25, 0.75, 0.90]).to_dict()\n",
    "mad_z0 = np.median(np.abs(filtered_z0 - np.median(filtered_z0)))\n",
    "\n",
    "skew_z0 = filtered_z0.skew()\n",
    "kurt_z0 = filtered_z0.kurt()\n",
    "\n",
    "# Additional metrics\n",
    "outliers_above_05 = (filtered_z0 > 0.5).sum()\n",
    "outliers_above_1 = (filtered_z0 > 1.0).sum()\n",
    "below_zero = (filtered_z0 < 0).sum()\n",
    "\n",
    "# R² stats\n",
    "\n",
    "# Clean the R² values\n",
    "valid_r2 = pd.to_numeric(combined_data['z0_corr_r2'], errors='coerce')\n",
    "valid_r2 = valid_r2[valid_r2.notna() & (valid_r2 >= 0) & (valid_r2 <= 1)]\n",
    "\n",
    "mean_r2 = valid_r2.mean()\n",
    "std_r2 = valid_r2.std()\n",
    "frac_above_95 = (valid_r2 > 0.95).mean() * 100\n",
    "\n",
    "# Prepare results\n",
    "z0_stats = {\n",
    "    'count': len(filtered_z0),\n",
    "    'mean': mean_z0,\n",
    "    'median': median_z0,\n",
    "    'std': std_z0,\n",
    "    'iqr': iqr_z0,\n",
    "    'percentiles': percentiles,\n",
    "    'mad': mad_z0,\n",
    "    'skewness': skew_z0,\n",
    "    'kurtosis': kurt_z0,\n",
    "    'outliers_above_0.5': outliers_above_05,\n",
    "    'outliers_above_1.0': outliers_above_1,\n",
    "    'below_zero': below_zero,\n",
    "    'mean_r2': mean_r2,\n",
    "    'std_r2': std_r2,\n",
    "    'frac_r2_above_95': frac_above_95\n",
    "}\n",
    "\n",
    "print(\"Roughness Length Metrics:\")\n",
    "print(pd.DataFrame([z0_stats]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c7c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine filtered_z0 from the cleaned combined_data\n",
    "filtered_z0 = combined_data[\n",
    "    (combined_data['z0_corrected'] > 0) &\n",
    "    (combined_data['z0_corrected'] < 1.0) &\n",
    "    (combined_data['z0_corr_r2'] > 0.9)\n",
    "]['z0_corrected']\n",
    "\n",
    "# Plot the PDF\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(filtered_z0, bins=100, kde=True, stat='density', color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Roughness Length $z_0$ [m]', fontsize=20)\n",
    "plt.ylabel('Probability Density', fontsize=20)\n",
    "plt.title('PDF of Roughness Length Estimates ($R^2>0.9$)', fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "#plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72227a53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop rows with z0 > 1\n",
    "combined_data = combined_data[combined_data['z0'] <= 10]\n",
    "\n",
    "# Reset the index\n",
    "combined_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Plot z0 vs. time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(combined_data['TIMESTAMP'], combined_data['z0'], marker='o', color='blue')\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.ylabel('Roughness Length (z0) [m]', fontsize=14)\n",
    "plt.title('Roughness Length (z0) vs. Time (Filtered)', fontsize=16)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the roughness lengths in the DataFrame\n",
    "print(combined_data[['TIMESTAMP', 'z0', 'WindDir_D15463_Avg']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ced6f7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot z0 vs. time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(combined_data['TIMESTAMP'], combined_data['z0'], marker='o', color='blue',label='z0')\n",
    "plt.scatter(combined_data['TIMESTAMP'], combined_data['z0_corrected'], color='green', alpha=0.8, label='z0_correct')\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.ylabel('Roughness Length (z0) [m]', fontsize=14)\n",
    "plt.title('Roughness Length (z0) vs. Time', fontsize=16)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2552774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for high-quality fits\n",
    "high_quality = combined_data[combined_data['z0_corr_r2'] > 0.9]\n",
    "\n",
    "# --- 1) Time Series Plot ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(high_quality['TIMESTAMP'], high_quality['z0_corrected'], 'o', markersize=2)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Corrected Roughness Length z₀ [m]')\n",
    "plt.title('Time Series of Corrected Roughness Length (z₀), R² > 0.9')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 1. Filter for high-quality fits ---\n",
    "high_quality = combined_data[combined_data['z0_corr_r2'] > 0.9].copy()\n",
    "\n",
    "# --- 2. Identify outliers (z0 > 1 m) ---\n",
    "outliers = high_quality[high_quality['z0_corrected'] > 1]\n",
    "n_outliers = len(outliers)\n",
    "n_total = len(high_quality)\n",
    "pct_outliers = 100 * n_outliers / n_total\n",
    "\n",
    "print(f\"High-quality z0_corrected values: {n_total}\")\n",
    "print(f\"Outliers with z0_corrected > 1 m: {n_outliers} ({pct_outliers:.1f}%)\")\n",
    "\n",
    "# --- 3. Filter out outliers for KDE ---\n",
    "filtered = high_quality[high_quality['z0_corrected'] <= 1]\n",
    "\n",
    "# --- 4. KDE estimation ---\n",
    "z0_vals = filtered['z0_corrected'].dropna().values\n",
    "kde = gaussian_kde(z0_vals, bw_method='scott')  # You can tweak bw_method if needed\n",
    "\n",
    "x_grid = np.linspace(0, 1, 500)\n",
    "pdf = kde(x_grid)\n",
    "\n",
    "# --- 5. Plot the PDF ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_grid, pdf, lw=2)\n",
    "plt.xlabel('Roughness Length $z_0$ [m]', fontsize=13)\n",
    "plt.ylabel('Probability Density', fontsize=13)\n",
    "plt.title('PDF of $z_0$ (Corrected, $R^2 > 0.9$, $z_0 \\\\leq 1$)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e403da5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define bin edges and labels\n",
    "# Define wind direction bins and labels\n",
    "bin_edges = [0, 22.5, 67.5, 112.5, 157.5, 202.5, 247.5, 292.5, 337.5]\n",
    "bin_labels = ['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW']  # Remove the duplicate 'N'\n",
    "# Assign wind directions to bins\n",
    "combined_data['WindDir_Category'] = pd.cut(\n",
    "    combined_data['WindDir_D15463_Avg'], \n",
    "    bins=bin_edges, \n",
    "    labels=bin_labels, \n",
    "    include_lowest=True,  # Include the lowest value in the first bin\n",
    "    right=False           # Use left-inclusive intervals\n",
    ")\n",
    "\n",
    "# Filter valid z0 values (non-NaN and <= 1)\n",
    "valid_data = combined_data[combined_data['z0'] <= 10].dropna(subset=['z0', 'WindDir_Category'])\n",
    "\n",
    "# Extract z0 values for plotting\n",
    "z0_values = valid_data['z0']\n",
    "\n",
    "# Initialize the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Loop through each wind direction category and plot the KDE (PDF) for z0\n",
    "for category in bin_labels:\n",
    "    # Extract z0 values for the current wind direction category\n",
    "    z0_category_values = valid_data[valid_data['WindDir_Category'] == category]['z0']\n",
    "    \n",
    "    if len(z0_category_values) > 0:\n",
    "        # Calculate the PDF using Gaussian Kernel Density Estimation (KDE)\n",
    "        kde = gaussian_kde(z0_category_values)\n",
    "        x_vals = np.linspace(z0_category_values.min(), z0_category_values.max(), 500)\n",
    "        y_vals = kde(x_vals)\n",
    "\n",
    "        # Plot the PDF for this wind direction category\n",
    "        plt.plot(x_vals, y_vals, label=f'{category} (n={len(z0_category_values)})')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Roughness Length (z0)', fontsize=14)\n",
    "plt.ylabel('Probability Density', fontsize=14)\n",
    "plt.title('PDF of Roughness Length (z0) by Wind Direction', fontsize=16)\n",
    "plt.legend(title=\"Wind Direction\", fontsize=10)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2883f99e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define wind direction bins and labels\n",
    "bin_edges = [0, 22.5, 67.5, 112.5, 157.5, 202.5, 247.5, 292.5, 337.5]\n",
    "bin_labels = ['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW']  # Remove the duplicate 'N'\n",
    "\n",
    "# Example: Assuming 'combined_data' is the DataFrame with 'WindDir_D15463_Avg' and 'z0' columns\n",
    "# Assign wind directions to bins\n",
    "combined_data['WindDir_Category'] = pd.cut(\n",
    "    combined_data['WindDir_D15463_Avg'], \n",
    "    bins=bin_edges, \n",
    "    labels=bin_labels, \n",
    "    include_lowest=True,  # Include the lowest value in the first bin\n",
    "    right=False           # Use left-inclusive intervals\n",
    ")\n",
    "\n",
    "# Filter valid z0 values (non-NaN and <= 1)\n",
    "valid_data = combined_data[combined_data['z0_corrected'] <= 1].dropna(subset=['z0_corrected', 'WindDir_Category'])\n",
    "\n",
    "# Extract z0 values for plotting\n",
    "z0_values = valid_data['z0_corrected']\n",
    "\n",
    "# Initialize the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Loop through each wind direction category and plot the KDE (PDF) for z0\n",
    "for category in bin_labels:\n",
    "    # Extract z0 values for the current wind direction category\n",
    "    z0_category_values = valid_data[valid_data['WindDir_Category'] == category]['z0_corrected']\n",
    "    \n",
    "    if len(z0_category_values) > 0:\n",
    "        # Calculate the PDF using Gaussian Kernel Density Estimation (KDE)\n",
    "        kde = gaussian_kde(z0_category_values)\n",
    "\n",
    "        # The KDE automatically smooths the data\n",
    "        kde_values = kde(z0_category_values)  # Evaluate the KDE at the data points\n",
    "\n",
    "        # Sort the z0 values for plotting the KDE\n",
    "        sorted_z0 = np.sort(z0_category_values)\n",
    "\n",
    "        # Plot the KDE using the actual data points\n",
    "        plt.plot(sorted_z0, kde(sorted_z0), label=f'{category} (n={len(z0_category_values)})')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Roughness Length (z0)', fontsize=14)\n",
    "plt.ylabel('Probability Density', fontsize=14)\n",
    "plt.title('PDF of Roughness Length (z0) by Wind Direction', fontsize=16)\n",
    "plt.legend(title=\"Wind Direction\", fontsize=10)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1d5cbf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 1) Define the two roughness‐estimation routines ----\n",
    "\n",
    "def estimate_z0_loglaw(heights, speeds, kappa=0.4):\n",
    "    \"\"\"Monin–Obukhov log‐law fit: returns (u_star, z0).\"\"\"\n",
    "    x = np.log(heights)\n",
    "    y = speeds\n",
    "    A, B = np.polyfit(x, y, 1)\n",
    "    u_star = A * kappa\n",
    "    z0 = np.exp(-B / A)\n",
    "    return u_star, z0\n",
    "\n",
    "def estimate_z0_powerlaw(heights, speeds, ref_height=10.0):\n",
    "    \"\"\"Empirical power‐law fit: returns (b, u_ref, z0).\"\"\"\n",
    "    x = np.log(heights / ref_height)\n",
    "    y = np.log(speeds)\n",
    "    b, ln_u_ref = np.polyfit(x, y, 1)\n",
    "    u_ref = np.exp(ln_u_ref)\n",
    "    z0 = np.exp(-(ln_u_ref - b * np.log(ref_height)) / b)\n",
    "    return b, u_ref, z0\n",
    "\n",
    "# --- 2) Vector of your measurement heights (m) ---\n",
    "heights = np.array([2.0, 4.47, 10.0])\n",
    "\n",
    "# --- 3) Apply to each row of combined_data ----\n",
    "\n",
    "def compute_roughness(row):\n",
    "    # pull the three heights' wind speeds\n",
    "    speeds = np.array([\n",
    "        row['WS_ms_D15008_Avg'],  # 2 m\n",
    "        row['WS_ms_D15014_Avg'],  # 4.47 m\n",
    "        row['WS_ms_D15463_Avg']    # 10 m\n",
    "    ], dtype=float)\n",
    "    \n",
    "    # if any are NaN or nonpositive, skip\n",
    "    if not np.all(np.isfinite(speeds)) or np.any(speeds <= 0):\n",
    "        return pd.Series({\n",
    "            'u_star_2': np.nan,\n",
    "            'z0_log': np.nan,\n",
    "            'b':       np.nan,\n",
    "            'u_ref':   np.nan,\n",
    "            'z0_power':np.nan\n",
    "        })\n",
    "    \n",
    "    # do the fits\n",
    "    u_star, z0_log = estimate_z0_loglaw(heights, speeds)\n",
    "    b, u_ref, z0_power = estimate_z0_powerlaw(heights, speeds)\n",
    "    \n",
    "    return pd.Series({\n",
    "        'u_star_2':   u_star,\n",
    "        'z0_log':   z0_log,\n",
    "        'b':        b,\n",
    "        'u_ref':    u_ref,\n",
    "        'z0_power': z0_power\n",
    "    })\n",
    "\n",
    "# run the vectorized apply\n",
    "rough = combined_data.apply(compute_roughness, axis=1)\n",
    "\n",
    "# merge the new columns back in\n",
    "combined_data = pd.concat([combined_data, rough], axis=1)\n",
    "\n",
    "# --- 4) Inspect the new columns ----\n",
    "#print(combined_data[['TIMESTAMP','u_star','z0_log','b','u_ref','z0_power']].head())\n",
    "# --- 1) Count how many z0 > 10 for each method ---\n",
    "n_log_exceed   = (combined_data['z0_log']   > 10).sum()\n",
    "n_power_exceed = (combined_data['z0_power'] > 10).sum()\n",
    "\n",
    "print(f\"Number of log‐law z0 > 10 m:   {n_log_exceed}\")\n",
    "print(f\"Number of power‐law z0 > 10 m: {n_power_exceed}\")\n",
    "\n",
    "# --- 2) Exclude those outliers before plotting ---\n",
    "combined_data = combined_data[\n",
    "    (combined_data['z0_log']   <= 10) &\n",
    "    (combined_data['z0_power'] <= 10)\n",
    "].copy()\n",
    "\n",
    "# (Optional) reset index on the filtered set\n",
    "combined_data.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4a3e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter both datasets for valid values and high fit quality\n",
    "filtered_corr = combined_data[\n",
    "    (combined_data['z0_corrected'] > 0) &\n",
    "    (combined_data['z0_corrected'] < 1.0) &\n",
    "    (combined_data['z0_corr_r2'] > 0.9)\n",
    "]['z0_corrected']\n",
    "\n",
    "filtered_log = combined_data[\n",
    "    (combined_data['z0_log'] > 0) &\n",
    "    (combined_data['z0_log'] < 1.0)\n",
    "]['z0_log']\n",
    "\n",
    "# Plot the PDFs\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(filtered_corr, bins=100, kde=True, stat='density', color='skyblue', edgecolor='black', label='Stability-corrected $z_0$', alpha=0.6)\n",
    "sns.histplot(filtered_log, bins=100, kde=True, stat='density', color='orange', edgecolor='black', label='Neutral log-law $z_0$', alpha=0.3)\n",
    "\n",
    "plt.xlabel('Roughness Length $z_0$ [m]', fontsize=20)\n",
    "plt.ylabel('Probability Density', fontsize=20)\n",
    "plt.title('PDF of Roughness Length Estimates ($R^2>0.9$)', fontsize=20)\n",
    "plt.legend(fontsize=14)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d89578a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter data: keep only z0 < 1 m\n",
    "filtered_data = combined_data[(combined_data['z0_log'] < 1) & (combined_data['z0_corrected'] < 1)]\n",
    "\n",
    "# Set plot style\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Plot KDE for uncorrected z0\n",
    "sns.kdeplot(\n",
    "    data=filtered_data,\n",
    "    x='z0_log',\n",
    "    bw_method='scott',  # can adjust to 'silverman' or float for sensitivity\n",
    "    label='Uncorrected (log-law)',\n",
    "    linestyle='--'\n",
    ")\n",
    "\n",
    "# Plot KDE for corrected z0\n",
    "sns.kdeplot(\n",
    "    data=filtered_data,\n",
    "    x='z0_corrected',\n",
    "    bw_method='scott',\n",
    "    label='Corrected (stability-adjusted)',\n",
    "    linestyle='-'\n",
    ")\n",
    "\n",
    "# Labels and legend\n",
    "plt.xlabel('Roughness Length $z_0$ [m]', fontsize=18)\n",
    "plt.ylabel('Probability Density', fontsize=18)\n",
    "plt.title('PDF of Roughness Length Estimates ($R^2>0.9$, $z_0 < 1$ m)', fontsize=20)\n",
    "plt.legend()\n",
    "plt.tick_params(labelsize=18)\n",
    "#plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312345f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(filtered_data['z0_log'], bins=70, density=True, alpha=0.6, label='Uncorrected (log-law)', histtype='step')\n",
    "plt.hist(filtered_data['z0_corrected'], bins=70, density=True, alpha=0.6, label='Corrected (stability-adjusted)', histtype='stepfilled')\n",
    "plt.xlabel('Roughness Length $z_0$ [m]', fontsize=12)\n",
    "plt.ylabel('Probability Density', fontsize=12)\n",
    "plt.title('Histogram-Based PDF of Roughness Length (z0 < 1 m)', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ab7359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import lognorm\n",
    "\n",
    "# Fit log-normal to corrected z0\n",
    "shape, loc, scale = lognorm.fit(filtered_data['z0_corrected'], floc=0)\n",
    "x_vals = np.linspace(0.001, 1, 200)\n",
    "pdf_vals = lognorm.pdf(x_vals, shape, loc=loc, scale=scale)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_vals, pdf_vals, label='Log-Normal Fit (Corrected)', color='orange')\n",
    "plt.xlabel('Roughness Length $z_0$ [m]', fontsize=12)\n",
    "plt.ylabel('Probability Density', fontsize=12)\n",
    "plt.title('Parametric PDF Fit to $z_0$ (Corrected)', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e45874",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Prepare heights and design matrix\n",
    "heights = np.array([2.0, 4.47, 10.0])\n",
    "ln_h = np.log(heights).reshape(-1, 1)\n",
    "\n",
    "r2_list = []\n",
    "\n",
    "for _, row in combined_data.iterrows():\n",
    "    # Extract and coerce to float\n",
    "    speeds = np.array([\n",
    "        row['WS_ms_D15008_Avg'],\n",
    "        row['WS_ms_D15014_Avg'],\n",
    "        row['WS_ms_D15463_Avg']\n",
    "    ], dtype=float)\n",
    "    \n",
    "    # Skip invalid or nonpositive\n",
    "    if np.any(~np.isfinite(speeds)) or np.any(speeds <= 0):\n",
    "        continue\n",
    "    \n",
    "    # Fit the 3‐point log‐law\n",
    "    model = LinearRegression().fit(ln_h, speeds)\n",
    "    r2 = model.score(ln_h, speeds)\n",
    "    r2_list.append(r2)\n",
    "\n",
    "# Summary\n",
    "print(f\"Number of successful fits: {len(r2_list)}\")\n",
    "print(f\"Mean per‐timestamp R²: {np.mean(r2_list):.3f}\")\n",
    "\n",
    "# Plot histogram of R² values\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(r2_list, bins=20, edgecolor='k', alpha=0.7)\n",
    "plt.xlabel('$R^2$ per timestamp', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.title('Distribution of Log‐Law Fit Quality', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b021a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r2_arr = np.array(r2_list)\n",
    "n_high_quality = np.sum(r2_arr > 0.9)\n",
    "pct_high_quality = 100 * n_high_quality / len(r2_arr)\n",
    "\n",
    "print(f\"Fits with R² > 0.9: {n_high_quality} out of {len(r2_arr)} \"\n",
    "      f\"({pct_high_quality:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8f09bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- 1) Compute and store r2 per row ---\n",
    "heights = np.array([2.0, 4.47, 10.0])\n",
    "ln_h = np.log(heights).reshape(-1,1)\n",
    "\n",
    "r2_vals = []\n",
    "for _, row in combined_data.iterrows():\n",
    "    speeds = np.array([\n",
    "        row['WS_ms_D15008_Avg'],\n",
    "        row['WS_ms_D15014_Avg'],\n",
    "        row['WS_ms_D15463_Avg']\n",
    "    ], dtype=float)\n",
    "    if np.any(~np.isfinite(speeds)) or np.any(speeds <= 0):\n",
    "        r2_vals.append(np.nan)\n",
    "    else:\n",
    "        r2 = LinearRegression().fit(ln_h, speeds).score(ln_h, speeds)\n",
    "        r2_vals.append(r2)\n",
    "\n",
    "combined_data['r2_loglaw'] = r2_vals\n",
    "\n",
    "# --- 2) Filter for high‐quality fits ---\n",
    "hq = combined_data[combined_data['r2_loglaw'] > 0.9].copy()\n",
    "\n",
    "# --- 3) Styled plot of z0_log for R² > 0.9 ---\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.scatter(\n",
    "    hq['TIMESTAMP'], hq['z0_log'],\n",
    "    s=40, alpha=0.8, marker='o',\n",
    "    edgecolor='k', linewidth=0.6,\n",
    "    label='Log‐law $z_0$ (R² > 0.9)'\n",
    ")\n",
    "\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "ax.xaxis.set_minor_locator(mdates.WeekdayLocator(byweekday=mdates.MO))\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha='right', fontsize=12)\n",
    "\n",
    "ax.set_xlabel('Date', fontsize=14)\n",
    "ax.set_ylabel('Roughness Length $z_0$ (m)', fontsize=14)\n",
    "ax.set_title('High‐Quality Log‐Law Roughness Estimates', fontsize=16, weight='bold')\n",
    "\n",
    "leg = ax.legend(frameon=True, fontsize=12)\n",
    "leg.get_frame().set_alpha(0.9)\n",
    "\n",
    "ax.grid(which='major', linestyle='--', linewidth=0.6, alpha=0.7)\n",
    "for spine in ['top','right']:\n",
    "    ax.spines[spine].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2baa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Define eight 45° sectors\n",
    "bin_edges = np.arange(0, 361, 45)   # [0,45,90,...,360]\n",
    "bin_labels = ['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW']\n",
    "\n",
    "# 2) Assign each timestamp to a sector\n",
    "combined_data['WindDir_Category'] = pd.cut(\n",
    "    combined_data['WindDir_D15463_Avg'] % 360,\n",
    "    bins=bin_edges,\n",
    "    labels=bin_labels,\n",
    "    include_lowest=True,\n",
    "    right=False\n",
    ")\n",
    "\n",
    "# 3) Filter for high-quality log-law z0\n",
    "#hq = combined_data[\n",
    " #   (combined_data['r2_loglaw'] > 0.9) &\n",
    "  #  (combined_data['z0_log'] > 0) &\n",
    "   # (combined_data['z0_log'] <= 1)\n",
    "#].dropna(subset=['WindDir_Category'])\n",
    "\n",
    "# 4) Plot KDEs by sector\n",
    "plt.figure(figsize=(12, 8))\n",
    "for sector in bin_labels:\n",
    "    vals = hq.loc[hq['WindDir_Category'] == sector, 'z0_log']\n",
    "    if len(vals) < 5:\n",
    "        continue\n",
    "    kde = gaussian_kde(vals)\n",
    "    xs = np.linspace(vals.min(), vals.max(), 200)\n",
    "    plt.plot(xs, kde(xs), label=f\"{sector} (n={len(vals)})\")\n",
    "\n",
    "plt.xlabel('Roughness Length $z_0$ (m)', fontsize=14)\n",
    "plt.ylabel('Probability Density', fontsize=14)\n",
    "plt.title('PDF of High-Quality $z_0$ by Wind Direction', fontsize=16, weight='bold')\n",
    "plt.legend(title='Wind Direction', fontsize=10)\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6c8da8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract the z0 values\n",
    "z0_vals = hq['z0_log'].values\n",
    "\n",
    "# 1) Compute KDE over a fine grid\n",
    "kde = gaussian_kde(z0_vals)\n",
    "x_grid = np.linspace(z0_vals.min(), z0_vals.max(), 200)\n",
    "kde_vals = kde(x_grid)\n",
    "\n",
    "# 2) Plot histogram + KDE\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Histogram (normalized to form a density)\n",
    "plt.hist(\n",
    "    z0_vals,\n",
    "    bins=30,\n",
    "    density=True,\n",
    "    edgecolor='k',\n",
    "    alpha=0.4,\n",
    "    label=f'Histogram (n={len(z0_vals)})'\n",
    ")\n",
    "\n",
    "# KDE curve\n",
    "plt.plot(\n",
    "    x_grid,\n",
    "    kde_vals,\n",
    "    color='darkorange',\n",
    "    linewidth=2,\n",
    "    label='Gaussian KDE'\n",
    ")\n",
    "\n",
    "plt.xlabel('Roughness Length $z_0$ (m)', fontsize=14)\n",
    "plt.ylabel('Probability Density', fontsize=14)\n",
    "plt.title('Distribution of High-Quality Roughness Lengths $(R^2 > 0.9)$', fontsize=16, weight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4f6000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Basic Statistics\n",
    "count = len(z0_vals)\n",
    "mean_z0 = np.mean(z0_vals)\n",
    "median_z0 = np.median(z0_vals)\n",
    "std_z0 = np.std(z0_vals, ddof=0)         # population std\n",
    "iqr_z0 = np.percentile(z0_vals, 75) - np.percentile(z0_vals, 25)\n",
    "\n",
    "p10, p25, p75, p90 = np.percentile(z0_vals, [10, 25, 75, 90])\n",
    "\n",
    "# 2) Robust Statistics\n",
    "mad_z0 = np.median(np.abs(z0_vals - median_z0))\n",
    "skew_z0 = skew(z0_vals)\n",
    "kurt_z0 = kurtosis(z0_vals)  # excess kurtosis\n",
    "\n",
    "# 3) Fit-Quality Stats\n",
    "r2_vals = hq['r2_loglaw'].values\n",
    "mean_r2 = np.mean(r2_vals)\n",
    "std_r2 = np.std(r2_vals, ddof=0)\n",
    "fract_r2_gt_95 = np.mean(r2_vals > 0.95)\n",
    "\n",
    "# 4) Outlier Counts\n",
    "n_gt_0p0 = np.sum(z0_vals < 0.0)\n",
    "n_gt_0p5 = np.sum(z0_vals > 0.5)\n",
    "n_gt_1p0 = np.sum(z0_vals > 1.0)\n",
    "pct_gt_0p0 = 100 * n_gt_0p0 / count\n",
    "\n",
    "pct_gt_0p5 = 100 * n_gt_0p5 / count\n",
    "pct_gt_1p0 = 100 * n_gt_1p0 / count\n",
    "\n",
    "# 5) Print results\n",
    "print(f\"High‐Quality z0 (n = {count}):\")\n",
    "print(f\"  Mean = {mean_z0:.3f} m,  Median = {median_z0:.3f} m\")\n",
    "print(f\"  Std = {std_z0:.3f} m,  IQR = {iqr_z0:.3f} m\")\n",
    "print(f\"  10/25/75/90 percentiles = {p10:.3f}/{p25:.3f}/{p75:.3f}/{p90:.3f} m\")\n",
    "print(f\"  MAD = {mad_z0:.3f} m,  Skewness = {skew_z0:.2f},  Kurtosis = {kurt_z0:.2f}\")\n",
    "print()\n",
    "print(\"Fit‐Quality (R²):\")\n",
    "print(f\"  Mean R² = {mean_r2:.3f},  Std R² = {std_r2:.3f}\")\n",
    "print(f\"  Fraction R² > 0.95 = {fract_r2_gt_95*100:.1f}%\")\n",
    "print()\n",
    "print(\"Outliers:\")\n",
    "print(f\"  z0 < 0.0 m: {n_gt_0p0} ({pct_gt_0p0:.1f}%)\")\n",
    "\n",
    "print(f\"  z0 > 0.5 m: {n_gt_0p5} ({pct_gt_0p5:.1f}%)\")\n",
    "print(f\"  z0 > 1.0 m: {n_gt_1p0} ({pct_gt_1p0:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(z0_vals)\n",
    "\n",
    "# 1) Define bins and compute bin width\n",
    "bin_edges = np.linspace(z0_vals.min(), z0_vals.max(), 31)  # 30 bins\n",
    "bin_width = bin_edges[1] - bin_edges[0]\n",
    "\n",
    "# 2) Compute histogram as percentages\n",
    "#    weights = 100 / n for each value, so sum of weights in a bin gives percent\n",
    "weights = np.ones_like(z0_vals) * (100.0 / n)\n",
    "\n",
    "# 3) Compute KDE and scale to percentage–per–bin\n",
    "kde = gaussian_kde(z0_vals)\n",
    "x_grid = np.linspace(z0_vals.min(), z0_vals.max(), 200)\n",
    "# KDE(x) is density; multiply by 100 (to convert to percent) and by bin_width\n",
    "kde_percent = kde(x_grid) * 100.0 * bin_width\n",
    "\n",
    "# 4) Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Histogram with percentages on y-axis\n",
    "plt.hist(\n",
    "    z0_vals,\n",
    "    bins=bin_edges,\n",
    "    weights=weights,\n",
    "    edgecolor='k',\n",
    "    alpha=0.6,\n",
    "    label=f'Histogram (%; n={n})'\n",
    ")\n",
    "\n",
    "# KDE curve (percentage scale)\n",
    "plt.plot(\n",
    "    x_grid,\n",
    "    kde_percent,\n",
    "    color='darkorange',\n",
    "    linewidth=2,\n",
    "    label='KDE (% scale)'\n",
    ")\n",
    "\n",
    "plt.xlabel('Roughness Length $z_0$ (m)', fontsize=14)\n",
    "plt.ylabel('Percentage of Observations (%)', fontsize=14)\n",
    "plt.title('Percentage Distribution of High-Quality Roughness Lengths $(R^2 > 0.9)$', fontsize=16, weight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
